<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Calculating Entropy &#8212; Bumps 0.9.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/nature.css?v=601dbdee" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css" />
    <script src="../_static/documentation_options.js?v=9dc39874"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Reference: bumps" href="../api/index.html" />
    <link rel="prev" title="Bumps Options" href="options.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../api/index.html" title="Reference: bumps"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="options.html" title="Bumps Options"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Bumps 0.9.1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="index.html" accesskey="U">User’s Guide</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Calculating Entropy</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="calculating-entropy">
<span id="entropy-guide"></span><h1>Calculating Entropy<a class="headerlink" href="#calculating-entropy" title="Link to this heading">¶</a></h1>
<p>Entropy is a measure of how much uncertainty is in the parameters.   We can
start with the simple case of a discrete parameter which can take on limited
set of values. Using the formula for discrete entropy:</p>
<div class="math notranslate nohighlight">
\[H(x) = - \sum_x p(x) \log_2(x)\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> is the set of possible states of the parameter, we can examine a
simple system with four states of equal probability:</p>
<img alt="../_images/entropy-discrete.png" src="../_images/entropy-discrete.png" />
<p>Before the experiment, the entropy is <span class="math notranslate nohighlight">\(-4 (1/4) \log_2(1/4) = 2\)</span> bits.  After
the experiment, which eliminates the states on the right, only two states are
remaining with an entropy of 1 bit.  The difference in entropy before and
after the experiment is the information gain, which is 1 bit in this case.</p>
<p>Extending this concept to continuous parameters, we use:</p>
<div class="math notranslate nohighlight">
\[h(x) = - \int_{x \in X} p(x) \log_2(x) dx\]</div>
<p>For a parameter which is normally distributed, <span class="math notranslate nohighlight">\(x \sim N(\mu, \sigma)\)</span>, the
entropy is:</p>
<div class="math notranslate nohighlight">
\[h(x) = \tfrac12 \log_2 (2 \pi e \sigma^2)\]</div>
<p>Consider an experiment in which the parameter uncertainty <span class="math notranslate nohighlight">\(\sigma\)</span> is reduced
from <span class="math notranslate nohighlight">\(\sigma=1\)</span> before the experiment to <span class="math notranslate nohighlight">\(\sigma=\tfrac12\)</span> after the
experiment:</p>
<img alt="../_images/entropy-continuous.png" src="../_images/entropy-continuous.png" />
<p>This experiment reduces the entropy from 2.05 bits to 1.05 bits, for an
information gain of 1 bit.</p>
<p>For a multivariate normal <span class="math notranslate nohighlight">\(N(\bar\mu, \Sigma)\)</span>, the entropy is</p>
<div class="math notranslate nohighlight">
\[h(N) = \tfrac{n}{2} \log_2 (2 \pi e) + \tfrac12 \log_2 \lvert \Sigma \rvert\]</div>
<p>where <span class="math notranslate nohighlight">\(n\)</span> is the number of fitting parameters and <span class="math notranslate nohighlight">\(\Sigma\)</span> is the
covariance matrix relating the parameters.  For an uncorrelated system, this
is proportional to <span class="math notranslate nohighlight">\(\sum_{i=1}^n \log_2 \sigma_i\)</span>, with the individual parameter
uncertainties <span class="math notranslate nohighlight">\(\sigma_i\)</span>. In effect, the entropy is a measure of overall
uncertainty resulting after the fit.</p>
<p>Within bumps, most models start with a uniform prior distribution for the
parameters set using the <em>x.range(low,high)</em> or <em>x.pm(delta)</em> for some
parameter <em>x</em>.  Some models set the prior probability to a normal distribution
using <em>x.dev(sigma)</em>.  Arbitrary prior probability distributions can be
set using <em>x.bounds = Distribution(D)</em> where <em>D</em> is a distribution following
the <em>scipy.stats</em> interface.  The uncertainty on the data points does not
directly enter into the entropy calculation.  Instead, it has a direct
influence on the calculation of the probability of seeing the data given
the parameter, and so it influences the probability of the parameters after
the fit.  Increasing the error bars will increase the variance in the
parameter estimation which will increase the entropy.</p>
<p>There are three ways that bumps can evaluate entropy.  For the fitters
which return a sample from the posterior distribution, such as DREAM,
BUMPS can estimate the entropy directly from the sample.  If the distribution
is approximately normal, we can compute the covariance matrix from the sample
and use the formula above for the multivariate normal.   For the remaining
fitters, we can use an estimate of the covariance matrix that results from
the fit (Levenberg-Marquardt, BFGS), or we can compute the Hessian at the
minimum (differential evolution, Nelder-Mead simplex).  Again, this can be
used in the formula above to give an estimate of the entropy.</p>
<p>We can use the difference in entropy between fits for experimental design.
After setting up the model system, we can simulate a dataset using the
expected statistics from the experiment, then fit the simulated data.  This
will give us the the expected uncertainty on our individual parameters, and
the overall entropy.  We can then play with different experimental parameters
such as instrument configurations, sample variants and measurement time and
select a combination which provides the most information about the parameters
of interest.  This can be done from the command line using
<a class="reference internal" href="options.html#option-simulate"><span class="std std-ref">--simulate</span></a>, <a class="reference internal" href="options.html#option-noise"><span class="std std-ref">--noise</span></a> and <a class="reference internal" href="options.html#option-entropy"><span class="std std-ref">--entropy</span></a>.</p>
<p>The information gain from the fit is not quite meaningful.  We can calculate
the prior entropy by looking at the fitting range of the parameters, and the
particular choice of fitting ranges can alter the output of the fit.  So for
example, if we set the fitting range to eliminate solutions, we will have
reduced the prior entropy as well as the posterior entropy, and likely
decreased the number of bits of information gain.  Conversely, if the fit
converges to the same distribution regardless of the parameter range, we can
drive the information gain to infinity by setting an unbounded input range.</p>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../index.html">
              <img class="logo" src="../_static/logo.png" alt="Logo"/>
            </a></p>
  <div>
    <h4>Previous topic</h4>
    <p class="topless"><a href="options.html"
                          title="previous chapter">Bumps Options</a></p>
  </div>
  <div>
    <h4>Next topic</h4>
    <p class="topless"><a href="../api/index.html"
                          title="next chapter">Reference: bumps</a></p>
  </div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/guide/entropy.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../api/index.html" title="Reference: bumps"
             >next</a> |</li>
        <li class="right" >
          <a href="options.html" title="Bumps Options"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Bumps 0.9.1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="index.html" >User’s Guide</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Calculating Entropy</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    </div>
  </body>
</html>