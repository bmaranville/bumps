<!DOCTYPE html>

<html lang="en" data-content_root="../../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>bumps.dream.entropy &#8212; Bumps 0.9.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../../_static/nature.css?v=601dbdee" />
    <link rel="stylesheet" type="text/css" href="../../../_static/plot_directive.css" />
    <script src="../../../_static/documentation_options.js?v=9dc39874"></script>
    <script src="../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">Bumps 0.9.1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../index.html" >Module code</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../../bumps.html" accesskey="U">bumps</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">bumps.dream.entropy</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for bumps.dream.entropy</h1><div class="highlight"><pre>
<span></span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Estimate entropy after a fit.</span>

<span class="sd">The :func:`gmm_entropy` function computes the entropy from a Gaussian mixture</span>
<span class="sd">model. This provides a reasonable estimate even for non-Gaussian distributions.</span>
<span class="sd">This is the recommended method for estimating the entropy of a sample.</span>

<span class="sd">The :func:`cov_entropy` method computes the entropy associated with the</span>
<span class="sd">covariance matrix.  This covariance matrix can be estimated during the</span>
<span class="sd">fitting procedure (BFGS updates an estimate of the Hessian matrix for example),</span>
<span class="sd">or computed by estimating derivatives when the fit is complete.</span>

<span class="sd">The :class:`MVNEntropy` class estimates the covariance from an MCMC sample and</span>
<span class="sd">uses this covariance to estimate the entropy.  This gives a better</span>
<span class="sd">estimate of the entropy than the equivalent direct calculation, which requires</span>
<span class="sd">many more samples for a good kernel density estimate.  The *reject_normal*</span>
<span class="sd">attribute is *True* if the MCMC sample is significantly different from normal.</span>
<span class="sd">Unfortunately, this almost always the case for any reasonable sample size that</span>
<span class="sd">isn&#39;t strictly gaussian.</span>

<span class="sd">The :func:`entropy` function computes the entropy directly from a set</span>
<span class="sd">of MCMC samples, normalized by a scale factor computed from the kernel density</span>
<span class="sd">estimate at a subset of the points.\ [#Kramer]_</span>

<span class="sd">There are many other entropy calculations implemented within this file, as</span>
<span class="sd">well as a number of sampling distributions for which the true entropy is known.</span>
<span class="sd">Furthermore, entropy was computed against dream output and checked for</span>
<span class="sd">consistency. None of the methods is truly excellent in terms of minimum</span>
<span class="sd">sample size, maximum dimensions and speed, but many of them are pretty</span>
<span class="sd">good.</span>

<span class="sd">The following is an informal summary of the results from different algorithms</span>
<span class="sd">applied to DREAM output::</span>

<span class="sd">        from .entropy import Timer as T</span>

<span class="sd">        # Try MVN ... only good for normal distributions, but very fast</span>
<span class="sd">        with T(): M = entropy.MVNEntropy(drawn.points)</span>
<span class="sd">        print(&quot;Entropy from MVN: %s&quot;%str(M))</span>

<span class="sd">        # Try wnn ... no good.</span>
<span class="sd">        with T(): S_wnn, Serr_wnn = entropy.wnn_entropy(drawn.points, n_est=20000)</span>
<span class="sd">        print(&quot;Entropy from wnn: %s&quot;%str(S_wnn))</span>

<span class="sd">        # Try wnn with bootstrap ... still no good.</span>
<span class="sd">        with T(): S_wnn, Serr_wnn = entropy.wnn_bootstrap(drawn.points)</span>
<span class="sd">        print(&quot;Entropy from wnn bootstrap: %s&quot;%str(S_wnn))</span>

<span class="sd">        # Try wnn entropy with thinning ... still no good.</span>
<span class="sd">        #drawn = self.draw(portion=portion, vars=vars,</span>
<span class="sd">        #                  selection=selection, thin=10)</span>
<span class="sd">        with T(): S_wnn, Serr_wnn = entropy.wnn_entropy(points)</span>
<span class="sd">        print(&quot;Entropy from wnn: %s&quot;%str(S_wnn))</span>

<span class="sd">        # Try wnn with gmm ... still no good</span>
<span class="sd">        with T(): S_wnn, Serr_wnn = entropy.wnn_entropy(drawn.points, n_est=20000, gmm=20)</span>
<span class="sd">        print(&quot;Entropy from wnn with gmm: %s&quot;%str(S_wnn))</span>

<span class="sd">        # Try pure gmm ... pretty good</span>
<span class="sd">        with T(): S_gmm, Serr_gmm = entropy.gmm_entropy(drawn.points, n_est=10000)</span>
<span class="sd">        print(&quot;Entropy from gmm: %s&quot;%str(S_gmm))</span>

<span class="sd">        # Try kde from statsmodels ... pretty good</span>
<span class="sd">        with T(): S_kde_stats = entropy.kde_entropy_statsmodels(drawn.points, n_est=10000)</span>
<span class="sd">        print(&quot;Entropy from kde statsmodels: %s&quot;%str(S_kde_stats))</span>

<span class="sd">        # Try kde from sklearn ... pretty good</span>
<span class="sd">        with T(): S_kde = entropy.kde_entropy_sklearn(drawn.points, n_est=10000)</span>
<span class="sd">        print(&quot;Entropy from kde sklearn: %s&quot;%str(S_kde))</span>

<span class="sd">        # Try kde from sklearn at points from gmm ... pretty good</span>
<span class="sd">        with T(): S_kde_gmm = entropy.kde_entropy_sklearn_gmm(drawn.points, n_est=10000)</span>
<span class="sd">        print(&quot;Entropy from kde+gmm: %s&quot;%str(S_kde_gmm))</span>

<span class="sd">        # Try Kramer ... pretty good, but doesn&#39;t support marginal entropy</span>
<span class="sd">        with T(): S, Serr = entropy.entropy(drawn.points, drawn.logp, N_entropy=n_est)</span>
<span class="sd">        print(&quot;Entropy from Kramer: %s&quot;%str(S))</span>


<span class="sd">.. [#Kramer]</span>
<span class="sd">    Kramer, A., Hasenauer, J., Allgower, F., Radde, N., 2010.</span>
<span class="sd">    Computation of the posterior entropy in a Bayesian framework</span>
<span class="sd">    for parameter estimation in biological networks,</span>
<span class="sd">    in: 2010 IEEE International Conference on Control Applications (CCA).</span>
<span class="sd">    Presented at the 2010 IEEE International Conference on</span>
<span class="sd">    Control Applications (CCA), pp. 493-498.</span>
<span class="sd">    doi:10.1109/CCA.2010.5611198</span>


<span class="sd">.. [#Turjillo-Ortiz]</span>
<span class="sd">    Trujillo-Ortiz, A. and R. Hernandez-Walls. (2003). Mskekur: Mardia&#39;s</span>
<span class="sd">        multivariate skewness and kurtosis coefficients and its hypotheses</span>
<span class="sd">        testing. A MATLAB file. [WWW document].</span>
<span class="sd">        `&lt;http://www.mathworks.com/matlabcentral/fileexchange/loadFile.do?objectId=3519&gt;`_</span>

<span class="sd">.. [#Mardia1970]</span>
<span class="sd">    Mardia, K. V. (1970), Measures of multivariate skewnees and kurtosis with</span>
<span class="sd">        applications. Biometrika, 57(3):519-530.</span>

<span class="sd">.. [#Mardia1974]</span>
<span class="sd">    Mardia, K. V. (1974), Applications of some measures of multivariate skewness</span>
<span class="sd">        and kurtosis for testing normality and robustness studies. Sankhy A,</span>
<span class="sd">        36:115-128</span>

<span class="sd">.. [#Stevens]</span>
<span class="sd">    Stevens, J. (1992), Applied Multivariate Statistics for Social Sciences.</span>
<span class="sd">        2nd. ed. New-Jersey:Lawrance Erlbaum Associates Publishers. pp. 247-248.</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span><span class="p">,</span> <span class="n">print_function</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;entropy&quot;</span><span class="p">,</span> <span class="s2">&quot;gmm_entropy&quot;</span><span class="p">,</span> <span class="s2">&quot;cov_entropy&quot;</span><span class="p">,</span> <span class="s2">&quot;wnn_entropy&quot;</span><span class="p">,</span> <span class="s2">&quot;MVNEntropy&quot;</span><span class="p">]</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="p">,</span> <span class="n">exp</span><span class="p">,</span> <span class="n">log</span><span class="p">,</span> <span class="n">sqrt</span><span class="p">,</span> <span class="n">log2</span><span class="p">,</span> <span class="n">pi</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="n">nan</span>
<span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">permutation</span><span class="p">,</span> <span class="n">choice</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span><span class="p">,</span> <span class="n">chi2</span>
<span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">gammaln</span><span class="p">,</span> <span class="n">digamma</span>
<span class="n">LN2</span> <span class="o">=</span> <span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">standardize</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Standardize the points by removing the mean and scaling by the standard</span>
<span class="sd">    deviation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: check if it is better to multiply by inverse covariance</span>
    <span class="c1"># That would serve to unrotate and unscale the dimensions together,</span>
    <span class="c1"># but squishing them down individually might be just as good.</span>

    <span class="c1"># compute zscores for the each variable independently</span>
    <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">std</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># Protect against NaN when sigma is zero.  If sigma is zero</span>
    <span class="c1"># then all points are equal, so x == mu and z-score is zero.</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">sigma</span> <span class="o">+</span> <span class="p">(</span><span class="n">sigma</span><span class="o">==</span><span class="mf">0.</span><span class="p">)),</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span>

<span class="k">def</span> <span class="nf">kde_entropy_statsmodels</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">n_est</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Use statsmodels KDEMultivariate pdf to estimate entropy.</span>

<span class="sd">    Density evaluated at sample points.</span>

<span class="sd">    Slow and fails for bimodal, dirichlet; poor for high dimensional MVN.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">statsmodels.nonparametric.kernel_density</span> <span class="kn">import</span> <span class="n">KDEMultivariate</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">points</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># Default to the full set</span>
    <span class="k">if</span> <span class="n">n_est</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">n_est</span> <span class="o">=</span> <span class="n">n</span>

    <span class="c1"># reduce size of draw to n_est</span>
    <span class="k">if</span> <span class="n">n_est</span> <span class="o">&gt;=</span> <span class="n">n</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">points</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">points</span><span class="p">[</span><span class="n">permutation</span><span class="p">(</span><span class="n">n</span><span class="p">)[:</span><span class="n">n_est</span><span class="p">]]</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">n_est</span>

    <span class="n">predictor</span> <span class="o">=</span> <span class="n">KDEMultivariate</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">var_type</span><span class="o">=</span><span class="s1">&#39;c&#39;</span><span class="o">*</span><span class="n">d</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">pdf</span><span class="p">()</span>
    <span class="n">H</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">H</span> <span class="o">/</span> <span class="n">LN2</span>

<span class="k">def</span> <span class="nf">kde_entropy_sklearn</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">n_est</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Use sklearn.neigbors.KernelDensity pdf to estimate entropy.</span>

<span class="sd">    Data is standardized before analysis.</span>

<span class="sd">    Sample points drawn from the kernel density estimate.</span>

<span class="sd">    Fails for bimodal and dirichlet, similar to statsmodels kde.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">points</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># Default to the full set</span>
    <span class="k">if</span> <span class="n">n_est</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">n_est</span> <span class="o">=</span> <span class="n">n</span>

    <span class="c1"># reduce size of draw to n_est</span>
    <span class="k">if</span> <span class="n">n_est</span> <span class="o">&gt;=</span> <span class="n">n</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">points</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">points</span><span class="p">[</span><span class="n">permutation</span><span class="p">(</span><span class="n">n</span><span class="p">)[:</span><span class="n">n_est</span><span class="p">]]</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">n_est</span>

    <span class="c1">#logp = sklearn_log_density(points, evaluation_points=n_est)</span>
    <span class="n">logp</span> <span class="o">=</span> <span class="n">sklearn_log_density</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">evaluation_points</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
    <span class="n">H</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">logp</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">H</span> <span class="o">/</span> <span class="n">LN2</span>

<span class="k">def</span> <span class="nf">kde_entropy_sklearn_gmm</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">n_est</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Use sklearn.neigbors.KernelDensity pdf to estimate entropy.</span>

<span class="sd">    Data is standardized before kde.</span>

<span class="sd">    Sample points drawn from gaussian mixture model from original points.</span>

<span class="sd">    Fails for bimodal and dirichlet, similar to statsmodels kde.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">sklearn.mixture</span> <span class="kn">import</span> <span class="n">BayesianGaussianMixture</span> <span class="k">as</span> <span class="n">GMM</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">points</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># Default to the full set</span>
    <span class="k">if</span> <span class="n">n_est</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">n_est</span> <span class="o">=</span> <span class="n">n</span>

    <span class="c1"># reduce size of draw to n_est</span>
    <span class="k">if</span> <span class="n">n_est</span> <span class="o">&gt;=</span> <span class="n">n</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">points</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">points</span><span class="p">[</span><span class="n">permutation</span><span class="p">(</span><span class="n">n</span><span class="p">)[:</span><span class="n">n_est</span><span class="p">]]</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">n_est</span>

    <span class="k">if</span> <span class="n">n_components</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">n_components</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">5</span><span class="o">*</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d</span><span class="p">))</span>

    <span class="n">predictor</span> <span class="o">=</span> <span class="n">GMM</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="s1">&#39;full&#39;</span><span class="p">,</span>
                    <span class="c1">#verbose=True,</span>
                    <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">predictor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">evaluation_points</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n_est</span><span class="p">)</span>

    <span class="n">logp</span> <span class="o">=</span> <span class="n">sklearn_log_density</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">evaluation_points</span><span class="o">=</span><span class="n">evaluation_points</span><span class="p">)</span>
    <span class="n">H</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">logp</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">H</span> <span class="o">/</span> <span class="n">LN2</span>

<div class="viewcode-block" id="gmm_entropy">
<a class="viewcode-back" href="../../../dream/entropy.html#bumps.dream.entropy.gmm_entropy">[docs]</a>
<span class="k">def</span> <span class="nf">gmm_entropy</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">n_est</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Use sklearn.mixture.BayesianGaussianMixture to estimate entropy.</span>

<span class="sd">    *points* are the data points in the sample.</span>

<span class="sd">    *n_est* are the number of points to use in the estimation; default is</span>
<span class="sd">    10,000 points, or 0 for all the points.</span>

<span class="sd">    *n_components* are the number of Gaussians in the mixture. Default is</span>
<span class="sd">    $5 \sqrt{d}$ where $d$ is the number of dimensions.</span>

<span class="sd">    Returns estimated entropy and uncertainty in the estimate.</span>

<span class="sd">    This method uses BayesianGaussianMixture from scikit-learn to build a</span>
<span class="sd">    model of the point distribution, then uses Monte Carlo sampling to</span>
<span class="sd">    determine the entropy of that distribution. The entropy uncertainty is</span>
<span class="sd">    computed from the variance in the MC sample scaled by the number of</span>
<span class="sd">    samples. This does not incorporate any uncertainty in the sampling that</span>
<span class="sd">    generated the point distribution or the uncertainty in the GMM used to</span>
<span class="sd">    model that distribution.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1">#from sklearn.mixture import GaussianMixture as GMM</span>
    <span class="kn">from</span> <span class="nn">sklearn.mixture</span> <span class="kn">import</span> <span class="n">BayesianGaussianMixture</span> <span class="k">as</span> <span class="n">GMM</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">points</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># Default to the full set</span>
    <span class="k">if</span> <span class="n">n_est</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">n_est</span> <span class="o">=</span> <span class="mi">10000</span>
    <span class="k">elif</span> <span class="n">n_est</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">n_est</span> <span class="o">=</span> <span class="n">n</span>

    <span class="c1"># reduce size of draw to n_est</span>
    <span class="k">if</span> <span class="n">n_est</span> <span class="o">&gt;=</span> <span class="n">n</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">points</span>
        <span class="n">n_est</span> <span class="o">=</span> <span class="n">n</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">points</span><span class="p">[</span><span class="n">permutation</span><span class="p">(</span><span class="n">n</span><span class="p">)[:</span><span class="n">n_est</span><span class="p">]]</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">n_est</span>

    <span class="k">if</span> <span class="n">n_components</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">n_components</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">5</span><span class="o">*</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d</span><span class="p">))</span>

    <span class="c1">## Standardization doesn&#39;t seem to help</span>
    <span class="c1">## Note: sigma may be zero</span>
    <span class="c1">#x, mu, sigma = standardize(x)   # if standardized</span>
    <span class="n">predictor</span> <span class="o">=</span> <span class="n">GMM</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="s1">&#39;full&#39;</span><span class="p">,</span>
                    <span class="c1">#verbose=True,</span>
                    <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">predictor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">eval_x</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n_est</span><span class="p">)</span>
    <span class="n">weight_x</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">score_samples</span><span class="p">(</span><span class="n">eval_x</span><span class="p">)</span>
    <span class="n">H</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">weight_x</span><span class="p">)</span>
    <span class="c1">#with np.errstate(divide=&#39;ignore&#39;): H = H + np.sum(np.log(sigma))   # if standardized</span>
    <span class="n">dH</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">weight_x</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="c1">## cross-check against own calcs</span>
    <span class="c1">#alt = GaussianMixture(predictor.weights_, mu=predictor.means_, sigma=predictor.covariances_)</span>
    <span class="c1">#print(&quot;alt&quot;, H, alt.entropy())</span>
    <span class="c1">#print(np.vstack((weight_x[:10], alt.logpdf(eval_x[:10]))).T)</span>
    <span class="k">return</span> <span class="n">H</span> <span class="o">/</span> <span class="n">LN2</span><span class="p">,</span> <span class="n">dH</span> <span class="o">/</span> <span class="n">LN2</span></div>


<span class="k">def</span> <span class="nf">wnn_bootstrap</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_est</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">parts</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="c1">#raise NotImplementedError(&quot;deprecated; bootstrap doesn&#39;t help.&quot;)</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">points</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">if</span> <span class="n">n_est</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">n_est</span> <span class="o">=</span> <span class="n">n</span><span class="o">//</span><span class="n">parts</span>

    <span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="n">wnn_entropy</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">n_est</span><span class="o">=</span><span class="n">n_est</span><span class="p">)</span>
               <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">reps</span><span class="p">)]</span>
    <span class="c1">#print(results)</span>
    <span class="n">S</span><span class="p">,</span> <span class="n">Serr</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">results</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">S</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>

<div class="viewcode-block" id="wnn_entropy">
<a class="viewcode-back" href="../../../dream/entropy.html#bumps.dream.entropy.wnn_entropy">[docs]</a>
<span class="k">def</span> <span class="nf">wnn_entropy</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_est</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">gmm</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Weighted Kozachenko-Leonenko nearest-neighbour entropy calculation.</span>

<span class="sd">    *k* is the number of neighbours to consider, with default $k=n^{1/3}$</span>

<span class="sd">    *n_est* is the number of points to use for estimating the entropy,</span>
<span class="sd">    with default $n_\rm{est} = n$</span>

<span class="sd">    *weights* is True for default weights, False for unweighted (using the</span>
<span class="sd">    distance to the kth neighbour only), or a vector of weights of length *k*.</span>

<span class="sd">    *gmm* is the number of gaussians to use to model the distribution using</span>
<span class="sd">    a gaussian mixture model.  Default is 0, and the points represent an</span>
<span class="sd">    empirical distribution.</span>

<span class="sd">    Returns entropy H in bits and its uncertainty.</span>

<span class="sd">    Berrett, T. B., Samworth, R.J., Yuan, M., 2016. Efficient multivariate</span>
<span class="sd">    entropy estimation via k-nearest neighbour distances.</span>
<span class="sd">    DOI:10.1214/18-AOS1688 https://arxiv.org/abs/1606.00304</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">NearestNeighbors</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">points</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># Default to the full set</span>
    <span class="k">if</span> <span class="n">n_est</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">n_est</span> <span class="o">=</span> <span class="mi">10000</span>
    <span class="k">elif</span> <span class="n">n_est</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">n_est</span> <span class="o">=</span> <span class="n">n</span>

    <span class="c1"># reduce size of draw to n_est</span>
    <span class="k">if</span> <span class="n">n_est</span> <span class="o">&gt;=</span> <span class="n">n</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">points</span>
        <span class="n">n_est</span> <span class="o">=</span> <span class="n">n</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">points</span><span class="p">[</span><span class="n">permutation</span><span class="p">(</span><span class="n">n</span><span class="p">)[:</span><span class="n">n_est</span><span class="p">]]</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">n_est</span>

    <span class="c1"># Default k based on n</span>
    <span class="k">if</span> <span class="n">k</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Private communication: cube root of n is a good choice for k</span>
        <span class="c1"># Personal observation: k should be much bigger than d</span>
        <span class="n">k</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">**</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">)),</span> <span class="mi">3</span><span class="o">*</span><span class="n">d</span><span class="p">)</span>

    <span class="c1"># If weights are given then use them (setting the appropriate k),</span>
    <span class="c1"># otherwise use the default weights.</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">_wnn_weights</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">k</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
    <span class="c1">#print(&quot;weights&quot;, weights, sum(weights))</span>

    <span class="c1"># select knn algorithm</span>
    <span class="n">algorithm</span> <span class="o">=</span> <span class="s1">&#39;auto&#39;</span>
    <span class="c1">#algorithm = &#39;kd_tree&#39;</span>
    <span class="c1">#algorithm = &#39;ball_tree&#39;</span>
    <span class="c1">#algorithm = &#39;brute&#39;</span>

    <span class="n">n_components</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">gmm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">gmm</span>

    <span class="c1"># H = 1/n sum_i=1^n sum_j=1^k w_j log E_{j,i}</span>
    <span class="c1"># E_{j,i} = e^-Psi(j) V_d (n-1) z_{j,i}^d = C z^d</span>
    <span class="c1"># logC = -Psi(j) + log(V_d) + log(n-1)</span>
    <span class="c1"># H = 1/n sum sum w_j logC + d/n sum sum w_j log(z)</span>
    <span class="c1">#   = sum w_j logC + d/n sum sum w_j log(z)</span>
    <span class="c1">#   = A + d/n B</span>
    <span class="c1"># H^2 = 1/n sum</span>
    <span class="n">Psi</span> <span class="o">=</span> <span class="n">digamma</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">logVd</span> <span class="o">=</span> <span class="n">d</span><span class="o">/</span><span class="mi">2</span><span class="o">*</span><span class="n">log</span><span class="p">(</span><span class="n">pi</span><span class="p">)</span> <span class="o">-</span> <span class="n">gammaln</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">d</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">logC</span> <span class="o">=</span> <span class="o">-</span><span class="n">Psi</span> <span class="o">+</span> <span class="n">logVd</span> <span class="o">+</span> <span class="n">log</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># TODO: standardizing points doesn&#39;t work.</span>
    <span class="c1"># Standardize the data so that distances conform.  This is equivalent to</span>
    <span class="c1"># a u-substitution u = sigma x + mu, so the integral needs to be corrected</span>
    <span class="c1"># for dU = det(sigma) dx.  Since the standardization squishes the dimensions</span>
    <span class="c1"># independently, sigma is a diagonal matrix, with the determinant equal to</span>
    <span class="c1"># the product of the diagonal elements.</span>
    <span class="c1">#x, mu, sigma = standardize(x)  # Note: sigma may be zero</span>
    <span class="c1">#detDU = np.prod(sigma)</span>
    <span class="n">detDU</span> <span class="o">=</span> <span class="mf">1.</span>

    <span class="k">if</span> <span class="n">n_components</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># Use Gaussian mixture to model the distribution</span>
        <span class="kn">from</span> <span class="nn">sklearn.mixture</span> <span class="kn">import</span> <span class="n">GaussianMixture</span> <span class="k">as</span> <span class="n">GMM</span>
        <span class="n">predictor</span> <span class="o">=</span> <span class="n">GMM</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">gmm</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="s1">&#39;full&#39;</span><span class="p">)</span>
        <span class="n">predictor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">eval_x</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n_est</span><span class="p">)</span>
        <span class="c1">#weight_x = predictor.score_samples(eval_x)</span>
        <span class="n">skip</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Empirical distribution</span>
        <span class="c1"># TODO: should we use the full draw for kNN and a subset for eval points?</span>
        <span class="c1"># Choose a subset for evaluating the entropy estimate, if desired</span>
        <span class="c1">#print(n_est, n)</span>
        <span class="c1">#eval_x = x if n_est &gt;= n else x[permutation(n)[:n_est]]</span>
        <span class="n">eval_x</span> <span class="o">=</span> <span class="n">x</span>
        <span class="c1">#weight_x = 1</span>
        <span class="n">skip</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="n">tree</span> <span class="o">=</span> <span class="n">NearestNeighbors</span><span class="p">(</span><span class="n">algorithm</span><span class="o">=</span><span class="n">algorithm</span><span class="p">,</span> <span class="n">n_neighbors</span><span class="o">=</span><span class="n">k</span><span class="o">+</span><span class="n">skip</span><span class="p">)</span>
    <span class="n">tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">dist</span><span class="p">,</span> <span class="n">_ind</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">kneighbors</span><span class="p">(</span><span class="n">eval_x</span><span class="p">,</span> <span class="n">n_neighbors</span><span class="o">=</span><span class="n">k</span><span class="o">+</span><span class="n">skip</span><span class="p">,</span> <span class="n">return_distance</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># Remove first column. Since test points are in x, the first column will</span>
    <span class="c1"># be a point from x with distance 0, and can be ignored.</span>
    <span class="k">if</span> <span class="n">skip</span><span class="p">:</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="n">dist</span><span class="p">[:,</span> <span class="n">skip</span><span class="p">:]</span>
    <span class="c1"># Find log distances.  This can be problematic for MCMC runs where a</span>
    <span class="c1"># step is rejected, and therefore identical points are in the distribution.</span>
    <span class="c1"># Ignore them by replacing these points with nan and using nanmean.</span>
    <span class="c1"># TODO: need proper analysis of duplicated points in MCMC chain</span>
    <span class="n">dist</span><span class="p">[</span><span class="n">dist</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">nan</span>
    <span class="n">logdist</span> <span class="o">=</span> <span class="n">log</span><span class="p">(</span><span class="n">dist</span><span class="p">)</span>
    <span class="n">H_unweighted</span> <span class="o">=</span> <span class="n">logC</span> <span class="o">+</span> <span class="n">d</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">logdist</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">H</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">H_unweighted</span><span class="p">,</span> <span class="n">weights</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">Hsq_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nanmean</span><span class="p">((</span><span class="n">logC</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">d</span><span class="o">*</span><span class="n">logdist</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="c1"># TODO: abs shouldn&#39;t be needed?</span>
    <span class="k">if</span> <span class="n">Hsq_k</span> <span class="o">&lt;</span> <span class="n">H</span><span class="o">**</span><span class="mi">2</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;warning: avg(H^2) &lt; avg(H)^2&quot;</span><span class="p">)</span>
    <span class="n">dH</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">Hsq_k</span> <span class="o">-</span> <span class="n">H</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">n_est</span><span class="p">)</span>
    <span class="c1">#print(&quot;unweighted&quot;, H_unweighted)</span>
    <span class="c1">#print(&quot;weighted&quot;, H, Hsq_k, H**2, dH, detDU, LN2)</span>
    <span class="k">return</span> <span class="n">H</span> <span class="o">*</span> <span class="n">detDU</span> <span class="o">/</span> <span class="n">LN2</span><span class="p">,</span> <span class="n">dH</span> <span class="o">*</span> <span class="n">detDU</span> <span class="o">/</span> <span class="n">LN2</span></div>


<span class="k">def</span> <span class="nf">_wnn_weights</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">weighted</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="c1"># Private communication: ignore w_j = 0 constraints (they are in the</span>
    <span class="c1"># paper for mathematical nicety), and find the L2 norm of the</span>
    <span class="c1"># remaining underdeterimined system described in Eq 2.</span>
    <span class="c1"># Personal observation: k should be some small multiple of d</span>
    <span class="c1"># otherwise the weights blow up.</span>
    <span class="k">if</span> <span class="n">d</span> <span class="o">&lt;</span> <span class="mi">4</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">weighted</span><span class="p">:</span>
        <span class="c1"># with few dimensions go unweighted with the kth nearest neighbour.</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mf">1.</span><span class="p">]])</span><span class="o">.</span><span class="n">T</span>
    <span class="n">j</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">sum_zero</span> <span class="o">=</span> <span class="p">[</span><span class="n">exp</span><span class="p">(</span><span class="n">gammaln</span><span class="p">(</span><span class="n">j</span><span class="o">+</span><span class="mi">2</span><span class="o">*</span><span class="n">i</span><span class="o">/</span><span class="n">d</span><span class="p">)</span><span class="o">-</span><span class="n">gammaln</span><span class="p">(</span><span class="n">j</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">d</span><span class="o">//</span><span class="mi">4</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span>
    <span class="n">sum_one</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">1.</span><span class="p">]</span><span class="o">*</span><span class="n">k</span><span class="p">]</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">sum_zero</span> <span class="o">+</span> <span class="n">sum_one</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">d</span><span class="o">//</span><span class="mi">4</span><span class="p">)</span><span class="o">+</span><span class="p">[</span><span class="mf">1.</span><span class="p">]])</span><span class="o">.</span><span class="n">T</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">A</span><span class="p">),</span> <span class="n">b</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">scipy_stats_density</span><span class="p">(</span><span class="n">sample_points</span><span class="p">,</span> <span class="n">evaluation_points</span><span class="p">):</span>  <span class="c1"># pragma: no cover</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Estimate the probability density function from which a set of sample</span>
<span class="sd">    points was drawn and return the estimated density at the evaluation points.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1">## standardize data so that we can use uniform bandwidth</span>
    <span class="c1">## Note: this didn&#39;t help with singular matrix</span>
    <span class="c1">## Note: if re-enable, protect against sigma=0 in some dimensions</span>
    <span class="c1">#mu, sigma = mean(data, axis=0), std(data, axis=0)</span>
    <span class="c1">#data,points = (data - mu)/sigma, (points - mu)/sigma</span>

    <span class="n">kde</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">gaussian_kde</span><span class="p">(</span><span class="n">sample_points</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">kde</span><span class="p">(</span><span class="n">evaluation_points</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">sklearn_log_density</span><span class="p">(</span><span class="n">sample_points</span><span class="p">,</span> <span class="n">evaluation_points</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Estimate the log probability density function from which a set of sample</span>
<span class="sd">    points was drawn and return the estimated density at the evaluation points.</span>

<span class="sd">    *sample_points* is an [n x m] matrix.</span>

<span class="sd">    *evaluation_points* is the set of points at which to evaluate the kde.</span>

<span class="sd">    Note: if any dimension has all points equal then the entire distribution</span>
<span class="sd">    is treated as a dirac distribution with infinite density at each point.</span>
<span class="sd">    This makes the entropy calculation better behaved (narrowing the</span>
<span class="sd">    distribution increases the entropy) but is not so useful in other contexts.</span>
<span class="sd">    Other packages will (correctly) ignore dimensions of width zero.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Ugly hack warning: if *evaluation_points* is an integer, then sample</span>
    <span class="c1"># that many points from the kde and return the log density at each</span>
    <span class="c1"># sampled point.  Since the code that uses this is looking only at</span>
    <span class="c1"># the mean log density, it doesn&#39;t need the sample points themselves.</span>
    <span class="c1"># This interface should be considered internal to the entropy module</span>
    <span class="c1"># and not used by outside functions.  If you need it externally, then</span>
    <span class="c1"># restructure the api so that the function always returns both the</span>
    <span class="c1"># points and the density, as well as any other function (such as the</span>
    <span class="c1"># denisty function and the sister function scipy_stats_density) so</span>
    <span class="c1"># that all share the new interface.</span>

    <span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KernelDensity</span>

    <span class="c1"># Standardize data so we can use spherical kernels and uniform bandwidth</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">standardize</span><span class="p">(</span><span class="n">sample_points</span><span class="p">)</span>

    <span class="c1"># Note that sigma will be zero for dimensions w_o where all points are equal.</span>
    <span class="c1"># With P(w) = P(w, w_o) / P(w_o | w) and P(w_o) = 1 for all points in</span>
    <span class="c1"># the set, then P(w) = P(w, w_o) and we can ignore the zero dimensions.</span>
    <span class="c1"># However, as another ugly hack, we want the differential entropy to go</span>
    <span class="c1"># to -inf as the distribution narrows, so pretend that P = 0 everywhere.</span>
    <span class="c1"># Uncomment the following line to return the sample probability instead.</span>
    <span class="c1">## sigma[sigma == 0.] = 1.</span>

    <span class="c1"># Silverman bandwidth estimator</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">sample_points</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">bandwidth</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="p">(</span><span class="n">d</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="mf">4.</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="n">d</span> <span class="o">+</span> <span class="mi">4</span><span class="p">))</span>

    <span class="c1">#print(&quot;starting grid search for bandwidth over %d points&quot;%n)</span>
    <span class="c1">#from sklearn.grid_search import GridSearchCV</span>
    <span class="c1">#from numpy import logspace</span>
    <span class="c1">#params = {&#39;bandwidth&#39;: logspace(-1, 1, 20)}</span>
    <span class="c1">#fitter = GridSearchCV(KernelDensity(), params)</span>
    <span class="c1">#fitter.fit(data)</span>
    <span class="c1">#kde = fitter.best_estimator_</span>
    <span class="c1">#print(&quot;best bandwidth: {0}&quot;.format(kde.bandwidth))</span>
    <span class="c1">#import time; T0 = time.time()</span>
    <span class="n">kde</span> <span class="o">=</span> <span class="n">KernelDensity</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;gaussian&#39;</span><span class="p">,</span> <span class="n">bandwidth</span><span class="o">=</span><span class="n">bandwidth</span><span class="p">,</span>
                        <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
    <span class="n">kde</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">evaluation_points</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="c1"># For generated points, they already follow the distribution</span>
        <span class="n">points</span> <span class="o">=</span> <span class="n">kde</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">evaluation_points</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Standardized evaluation points to match sample distribution</span>
        <span class="c1"># Note: for dimensions where all sample points are equal, sigma</span>
        <span class="c1"># has been artificially set equal to one.  This means that the</span>
        <span class="c1"># evaluation points which do not match the sample value will</span>
        <span class="c1"># use the simple differences for the z-score rather than</span>
        <span class="c1"># pushing them out to plus/minus infinity.</span>
        <span class="n">points</span> <span class="o">=</span> <span class="p">(</span><span class="n">evaluation_points</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">sigma</span> <span class="o">+</span> <span class="p">(</span><span class="n">sigma</span> <span class="o">==</span> <span class="mf">0.</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">points</span> <span class="o">=</span> <span class="n">sample_points</span>

    <span class="c1"># Evaluate pdf, scaling the resulting density by sigma to correct the area.</span>
    <span class="c1"># If sigma is zero, return entropy as -inf;  this seems to not be the</span>
    <span class="c1"># case for discrete distributions (consider Bernoulli with p=1, q=0,</span>
    <span class="c1">#  =&gt; H = -p log p - q log q = 0), so need to do something else, both</span>
    <span class="c1"># for the kde and for the entropy calculation.</span>
    <span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">errstate</span><span class="p">(</span><span class="n">divide</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">):</span>
        <span class="n">log_pdf</span> <span class="o">=</span> <span class="n">kde</span><span class="o">.</span><span class="n">score_samples</span><span class="p">(</span><span class="n">points</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">sigma</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">log_pdf</span>


<span class="k">def</span> <span class="nf">sklearn_density</span><span class="p">(</span><span class="n">sample_points</span><span class="p">,</span> <span class="n">evaluation_points</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Estimate the probability density function from which a set of sample</span>
<span class="sd">    points was drawn and return the estimated density at the evaluation points.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">exp</span><span class="p">(</span><span class="n">sklearn_log_density</span><span class="p">(</span><span class="n">sample_points</span><span class="p">,</span> <span class="n">evaluation_points</span><span class="p">))</span>


<span class="c1"># scipy kde fails with singular matrix, so we will use scikit.learn</span>
<span class="c1">#density = scipy_stats_density</span>
<span class="n">density</span> <span class="o">=</span> <span class="n">sklearn_density</span>


<div class="viewcode-block" id="entropy">
<a class="viewcode-back" href="../../../dream/entropy.html#bumps.dream.entropy.entropy">[docs]</a>
<span class="k">def</span> <span class="nf">entropy</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">logp</span><span class="p">,</span> <span class="n">N_entropy</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">N_norm</span><span class="o">=</span><span class="mi">2500</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return entropy estimate and uncertainty from a random sample.</span>

<span class="sd">    *points* is a set of draws from an underlying distribution, as returned</span>
<span class="sd">    by a Markov chain Monte Carlo process for example.</span>

<span class="sd">    *logp* is the log-likelihood for each draw.</span>

<span class="sd">    *N_norm* is the number of points $k$ to use to estimate the posterior</span>
<span class="sd">    density normalization factor $P(D) = \hat N$, converting</span>
<span class="sd">    from $\log( P(D|M) P(M) )$ to $\log( P(D|M)P(M)/P(D) )$. The relative</span>
<span class="sd">    uncertainty $\Delta\hat S/\hat S$ scales with $\sqrt{k}$, with the</span>
<span class="sd">    default *N_norm=2500* corresponding to 2% relative uncertainty.</span>
<span class="sd">    Computation cost is $O(nk)$ where $n$ is number of points in the draw.</span>

<span class="sd">    *N_entropy* is the number of points used to estimate the entropy</span>
<span class="sd">    $\hat S = - \int P(M|D) \log P(M|D)$ from the normalized log likelihood</span>
<span class="sd">    values.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Use a random subset to estimate density</span>
    <span class="k">if</span> <span class="n">N_norm</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">logp</span><span class="p">):</span>
        <span class="n">norm_points</span> <span class="o">=</span> <span class="n">points</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">permutation</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">points</span><span class="p">))[:</span><span class="n">N_entropy</span><span class="p">]</span>
        <span class="n">norm_points</span> <span class="o">=</span> <span class="n">points</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

    <span class="c1"># Use a different subset to estimate the scale factor between density</span>
    <span class="c1"># and logp.</span>
    <span class="k">if</span> <span class="n">N_entropy</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">N_entropy</span> <span class="o">=</span> <span class="mi">10000</span>
    <span class="k">if</span> <span class="n">N_entropy</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">logp</span><span class="p">):</span>
        <span class="n">entropy_points</span><span class="p">,</span> <span class="n">eval_logp</span> <span class="o">=</span> <span class="n">points</span><span class="p">,</span> <span class="n">logp</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">permutation</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">points</span><span class="p">))[:</span><span class="n">N_entropy</span><span class="p">]</span>
        <span class="n">entropy_points</span><span class="p">,</span> <span class="n">eval_logp</span> <span class="o">=</span> <span class="n">points</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">logp</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    # Try again, just using the points from the high probability regions</span>
<span class="sd">    # to determine the scale factor</span>
<span class="sd">    N_norm = min(len(logp), 5000)</span>
<span class="sd">    N_entropy = int(0.8*N_norm)</span>
<span class="sd">    idx = np.argsort(logp)</span>
<span class="sd">    norm_points = points[idx[-N_norm:]]</span>
<span class="sd">    entropy_points = points[idx[-N_entropy:]]</span>
<span class="sd">    eval_logp = logp[idx[-N_entropy:]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Normalize p to a peak probability of 1 so that exp() doesn&#39;t underflow.</span>
    <span class="c1">#</span>
    <span class="c1"># This should be okay since for the normalizing constant C:</span>
    <span class="c1">#</span>
    <span class="c1">#      u&#39; = e^(ln u + ln C) = e^(ln u)e^(ln C) = C u</span>
    <span class="c1">#</span>
    <span class="c1"># Using eq. 11 of Kramer with u&#39; substituted for u:</span>
    <span class="c1">#</span>
    <span class="c1">#      N_est = &lt; u&#39;/p &gt; = &lt; C u/p &gt; = C &lt; u/p &gt;</span>
    <span class="c1">#</span>
    <span class="c1">#      S_est = - &lt; ln q &gt;</span>
    <span class="c1">#            = - &lt; ln (u&#39;/N_est) &gt;</span>
    <span class="c1">#            = - &lt; ln C + ln u - ln (C &lt;u/p&gt;) &gt;</span>
    <span class="c1">#            = - &lt; ln u + ln C - ln C - ln &lt;u/p&gt; &gt;</span>
    <span class="c1">#            = - &lt; ln u - ln &lt;u/p&gt; &gt;</span>
    <span class="c1">#            = - &lt; ln u &gt; + ln &lt;u/p&gt;</span>
    <span class="c1">#</span>
    <span class="c1"># Uncertainty comes from eq. 13:</span>
    <span class="c1">#</span>
    <span class="c1">#      N_err^2 = 1/(k-1) sum( (u&#39;/p - &lt;u&#39;/p&gt;)^2 )</span>
    <span class="c1">#              = 1/(k-1) sum( (C u/p - &lt;C u/p&gt;)^2 )</span>
    <span class="c1">#              = C^2 std(u/p)^2</span>
    <span class="c1">#      S_err = std(u&#39;/p) / &lt;u&#39;/p&gt; = (C std(u/p))/(C &lt;u/p&gt;) = std(u/p)/&lt;u/p&gt;</span>
    <span class="c1">#</span>
    <span class="c1"># So even though the constant C shows up in N_est, N_err, it cancels</span>
    <span class="c1"># again when S_est, S_err is formed.</span>
    <span class="n">log_scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">eval_logp</span><span class="p">)</span>
    <span class="c1"># print(&quot;max log sample: %g&quot;%log_scale)</span>
    <span class="n">eval_logp</span> <span class="o">-=</span> <span class="n">log_scale</span>

    <span class="c1"># Compute entropy and uncertainty in nats</span>
    <span class="c1"># Note: if all values are the same in any dimension then we have a dirac</span>
    <span class="c1"># functional with infinite probability at every sample point, and the</span>
    <span class="c1"># differential entropy estimate will yield H = -inf.</span>
    <span class="n">rho</span> <span class="o">=</span> <span class="n">density</span><span class="p">(</span><span class="n">norm_points</span><span class="p">,</span> <span class="n">entropy_points</span><span class="p">)</span>
    <span class="c1">#print(rho.min(), rho.max(), eval_logp.min(), eval_logp.max())</span>
    <span class="n">frac</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="n">eval_logp</span><span class="p">)</span><span class="o">/</span><span class="n">rho</span>
    <span class="n">n_est</span><span class="p">,</span> <span class="n">n_err</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">frac</span><span class="p">),</span> <span class="n">std</span><span class="p">(</span><span class="n">frac</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">n_est</span> <span class="o">==</span> <span class="mf">0.</span><span class="p">:</span>
        <span class="n">s_est</span><span class="p">,</span> <span class="n">s_err</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="mf">0.</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">s_est</span> <span class="o">=</span> <span class="n">log</span><span class="p">(</span><span class="n">n_est</span><span class="p">)</span> <span class="o">-</span> <span class="n">mean</span><span class="p">(</span><span class="n">eval_logp</span><span class="p">)</span>
        <span class="n">s_err</span> <span class="o">=</span> <span class="n">n_err</span><span class="o">/</span><span class="n">n_est</span>
    <span class="c1">#print(n_est, n_err, s_est/LN2, s_err/LN2)</span>
    <span class="c1">##print(np.median(frac), log(np.median(frac))/LN2, log(n_est)/LN2)</span>
    <span class="k">if</span> <span class="kc">False</span><span class="p">:</span>
        <span class="kn">import</span> <span class="nn">pylab</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">pylab</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">entropy_points</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="n">pylab</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
        <span class="n">pylab</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">221</span><span class="p">)</span>
        <span class="n">pylab</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">points</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">pylab</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">entropy_points</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">rho</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;density&#39;</span><span class="p">)</span>
        <span class="n">pylab</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">entropy_points</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">exp</span><span class="p">(</span><span class="n">eval_logp</span><span class="o">+</span><span class="n">log_scale</span><span class="p">)[</span><span class="n">idx</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;p&#39;</span><span class="p">)</span>
        <span class="n">pylab</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;p(x)&quot;</span><span class="p">)</span>
        <span class="n">pylab</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="n">pylab</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">222</span><span class="p">)</span>
        <span class="n">pylab</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">points</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">pylab</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">entropy_points</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">rho</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;density&#39;</span><span class="p">)</span>
        <span class="n">pylab</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">entropy_points</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">exp</span><span class="p">(</span><span class="n">eval_logp</span><span class="o">+</span><span class="n">log_scale</span><span class="p">)[</span><span class="n">idx</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;p&#39;</span><span class="p">)</span>
        <span class="n">pylab</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;p(x)&quot;</span><span class="p">)</span>
        <span class="n">pylab</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="n">pylab</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">212</span><span class="p">)</span>
        <span class="n">pylab</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">entropy_points</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">frac</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>
        <span class="n">pylab</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;P[0] value&quot;</span><span class="p">)</span>
        <span class="n">pylab</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;p(x)/kernel density&quot;</span><span class="p">)</span>

    <span class="c1"># return entropy and uncertainty in bits</span>
    <span class="k">return</span> <span class="n">s_est</span><span class="o">/</span><span class="n">LN2</span><span class="p">,</span> <span class="n">s_err</span><span class="o">/</span><span class="n">LN2</span></div>



<div class="viewcode-block" id="MVNEntropy">
<a class="viewcode-back" href="../../../dream/entropy.html#bumps.dream.entropy.MVNEntropy">[docs]</a>
<span class="k">class</span> <span class="nc">MVNEntropy</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multivariate normal entropy approximation.</span>

<span class="sd">    Uses Mardia&#39;s multivariate skewness and kurtosis test to estimate normality.</span>

<span class="sd">    *x* is a set of points</span>

<span class="sd">    *alpha* is the cutoff for the normality test.</span>

<span class="sd">    *max_points* is the maximum number of points to use when checking</span>
<span class="sd">    normality.  Since the normality test is $O(n^2)$ in memory and time,</span>
<span class="sd">    where $n$ is the number of points, *max_points* defaults to 1000. The</span>
<span class="sd">    entropy is computed from the full dataset.</span>

<span class="sd">    The returned object has the following attributes:</span>

<span class="sd">        *p_kurtosis* is the p-value for the kurtosis normality test</span>

<span class="sd">        *p_skewness* is the p-value for the skewness normality test</span>

<span class="sd">        *reject_normal* is True if either the the kurtosis or the skew test</span>
<span class="sd">        fails</span>

<span class="sd">        *entropy* is the estimated entropy of the best normal approximation</span>
<span class="sd">        to the distribution</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: use robust covariance estimator for mean and covariance</span>
    <span class="c1"># FastMSD is available in sklearn.covariance.MinDetCov. There are methods</span>
    <span class="c1"># such as (Zhahg, 2012), which may be faster if performance is an issue.</span>
    <span class="c1"># [1] Zhang (2012) DOI: 10.5539/ijsp.v1n2p119</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">max_points</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
        <span class="c1"># compute Mardia test coefficient</span>
        <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>   <span class="c1"># num points, num dimensions</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">p</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)]])</span>
        <span class="c1"># squared Mahalanobis distance matrix</span>
        <span class="c1"># Note: this forms a full n x n matrix of distances, so will</span>
        <span class="c1"># fail for a large number of points.  Kurtosis only requires</span>
        <span class="c1"># the diagonal elements so can be computed cheaply.  If there</span>
        <span class="c1"># is no order to the points, skew could be estimated using only</span>
        <span class="c1"># the block diagonal</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:])[:</span><span class="n">max_points</span><span class="p">]</span>
        <span class="n">D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dx</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">dx</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
        <span class="n">kurtosis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">D</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">n</span>
        <span class="n">skewness</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">D</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span><span class="o">/</span><span class="n">n</span><span class="o">**</span><span class="mi">2</span>

        <span class="n">kurtosis_stat</span> <span class="o">=</span> <span class="p">(</span><span class="n">kurtosis</span> <span class="o">-</span> <span class="n">p</span><span class="o">*</span><span class="p">(</span><span class="n">p</span><span class="o">+</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="mi">8</span><span class="o">*</span><span class="n">p</span><span class="o">*</span><span class="p">(</span><span class="n">p</span><span class="o">+</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">n</span><span class="p">)</span>
        <span class="n">raw_skewness_stat</span> <span class="o">=</span> <span class="n">n</span><span class="o">*</span><span class="n">skewness</span><span class="o">/</span><span class="mi">6</span>
        <span class="c1"># Small sample correction converges to 1 as n increases, so it is</span>
        <span class="c1"># always safe to apply it</span>
        <span class="n">small_sample_correction</span> <span class="o">=</span> <span class="p">(</span><span class="n">p</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">n</span><span class="o">+</span><span class="mi">3</span><span class="p">)</span><span class="o">/</span><span class="p">((</span><span class="n">p</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">n</span> <span class="o">-</span> <span class="n">n</span><span class="o">*</span><span class="mi">6</span><span class="p">)</span>
        <span class="n">skewness_stat</span> <span class="o">=</span> <span class="n">raw_skewness_stat</span> <span class="o">*</span> <span class="n">small_sample_correction</span>
        <span class="n">dof</span> <span class="o">=</span> <span class="p">(</span><span class="n">p</span><span class="o">*</span><span class="p">(</span><span class="n">p</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">p</span><span class="o">+</span><span class="mi">2</span><span class="p">))</span><span class="o">/</span><span class="mi">6</span>   <span class="c1"># degrees of freedom for chisq test</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">p_kurtosis</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">kurtosis_stat</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p_skewness</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">chi2</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">skewness_stat</span><span class="p">,</span> <span class="n">dof</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reject_normal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">p_kurtosis</span> <span class="o">&lt;</span> <span class="n">alpha</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">p_skewness</span> <span class="o">&lt;</span> <span class="n">alpha</span>
        <span class="c1">#print(&quot;kurtosis&quot;, kurtosis, kurtosis_stat, self.p_kurtosis)</span>
        <span class="c1">#print(&quot;skewness&quot;, skewness, skewness_stat, self.p_skewness)</span>
        <span class="c1"># compute entropy</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">entropy</span> <span class="o">=</span> <span class="n">cov_entropy</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&quot;H=</span><span class="si">%.1f</span><span class="s2"> bits</span><span class="si">%s</span><span class="s2">&quot;</span><span class="o">%</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">entropy</span><span class="p">,</span> <span class="s2">&quot; (not normal)&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reject_normal</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="cov_entropy">
<a class="viewcode-back" href="../../../dream/entropy.html#bumps.dream.entropy.cov_entropy">[docs]</a>
<span class="k">def</span> <span class="nf">cov_entropy</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Entropy estimate from covariance matrix C</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">C</span><span class="p">)</span> <span class="o">*</span> <span class="n">log2</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">pi</span><span class="o">*</span><span class="n">e</span><span class="p">)</span> <span class="o">+</span> <span class="n">log2</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">C</span><span class="p">))))</span></div>



<span class="k">def</span> <span class="nf">mvn_entropy_bootstrap</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">samples</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Use bootstrap method to estimate entropy and its uncertainty</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">points</span><span class="o">.</span><span class="n">shape</span>

    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">samples</span><span class="p">):</span>
        <span class="c1"># sample n points with replacement in 0 ... n-1.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">points</span><span class="p">[</span><span class="n">choice</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)]</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">d</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)]])</span>
        <span class="c1">#print(f&quot;cov {samples}, {x.shape}, {C.shape}&quot;)</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cov_entropy</span><span class="p">(</span><span class="n">C</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">results</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

<span class="c1"># ======================================================================</span>
<span class="c1"># Testing code</span>
<span class="c1"># ======================================================================</span>

<span class="c1"># Based on: Eli Bendersky https://stackoverflow.com/a/5849861</span>
<span class="c1"># Extended with tic/toc by Paul Kienzle</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="k">class</span> <span class="nc">Timer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">tic</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">Timer</span><span class="p">(</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">toc</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_number</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tlast</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tstart</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">toc</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_number</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">step</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">step</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">step_number</span><span class="p">)</span>
        <span class="n">label</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="o">+</span> <span class="n">step</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="k">else</span> <span class="n">step</span>
        <span class="n">tnext</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">total</span> <span class="o">=</span> <span class="n">tnext</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">tstart</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="n">tnext</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">tlast</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;[</span><span class="si">%s</span><span class="s1">] Elapsed: </span><span class="si">%s</span><span class="s1">, Delta: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">total</span><span class="p">,</span> <span class="n">delta</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tlast</span> <span class="o">=</span> <span class="n">tnext</span>
    <span class="k">def</span> <span class="fm">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tlast</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tstart</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="k">def</span> <span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">type</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">traceback</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;[</span><span class="si">%s</span><span class="s1">]&#39;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Elapsed: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">tstart</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">entropy_mc</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">1000000</span><span class="p">):</span>
    <span class="n">logp</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">D</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">N</span><span class="p">))</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">logp</span><span class="p">)</span>
    <span class="c1">#return -np.mean(logp[np.isfinite(logp)])</span>

<span class="c1"># CRUFT: dirichlet needs transpose of theta for logpdf</span>
<span class="k">class</span> <span class="nc">Dirichlet</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dist</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">dirichlet</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">logpdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dist</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">rvs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dist</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">)</span>
        <span class="c1"># Dirichlet logpdf is failing if x=0 for any x when alpha&lt;1.</span>
        <span class="c1"># The simplex check allows fudge of 1e-10.</span>
        <span class="n">x</span><span class="p">[</span><span class="n">x</span><span class="o">==</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1e-100</span>
        <span class="k">return</span> <span class="n">x</span>
    <span class="k">def</span> <span class="nf">entropy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dist</span><span class="o">.</span><span class="n">entropy</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Box</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">width</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">width</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">center</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;d&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">center</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">center</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">width</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;d&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">center</span> <span class="o">=</span> <span class="n">center</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">width</span> <span class="o">=</span> <span class="n">width</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">width</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_logpdf</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">width</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">rvs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">width</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mf">0.5</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">width</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">center</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">logpdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">theta</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">center</span><span class="p">)</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">width</span> <span class="o">+</span> <span class="mf">0.5</span>
        <span class="n">logp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_logpdf</span>
        <span class="n">logp</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">y</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
        <span class="n">logp</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">y</span><span class="o">&gt;</span><span class="mi">1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
        <span class="k">return</span> <span class="n">logp</span>

    <span class="k">def</span> <span class="nf">entropy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">_logpdf</span>

<span class="c1"># CRUFT: scipy MVN gives wrong entropy for singular (and near singular) matrices</span>
<span class="c1"># This solution gives wrong results for near-singular rvs(), but for the simple</span>
<span class="c1"># case of a diagonal Sigma with one zero it does what I need for the test.</span>
<span class="k">class</span> <span class="nc">MVNSingular</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">):</span>
        <span class="n">kw</span><span class="p">[</span><span class="s1">&#39;allow_singular&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dist</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">dim</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">dim</span>

    <span class="k">def</span> <span class="nf">pdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">logpdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">rvs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">entropy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">10000</span><span class="p">):</span>
        <span class="c1"># CRUFT scipy==1.10.0: scipy.stats briefly removed the dist.cov attribute.</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="p">,</span> <span class="s1">&#39;cov&#39;</span><span class="p">):</span>
            <span class="n">cov</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">cov</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">cov</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">cov_object</span><span class="o">.</span><span class="n">covariance</span>

        <span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">errstate</span><span class="p">(</span><span class="n">divide</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">):</span>
            <span class="k">return</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="n">pi</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">e</span><span class="p">)</span><span class="o">*</span><span class="n">cov</span><span class="p">))</span>

<span class="k">class</span> <span class="nc">GaussianMixture</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">mu</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">sigma</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sigma</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>
        <span class="n">sigma</span> <span class="o">=</span> <span class="p">[(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span> <span class="k">if</span> <span class="n">s</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">s</span><span class="p">))</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sigma</span><span class="p">]</span>
        <span class="n">sigma</span> <span class="o">=</span> <span class="p">[(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sigma</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dist</span> <span class="o">=</span> <span class="p">[</span><span class="n">stats</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">m</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">s</span><span class="p">)</span>
                     <span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)]</span>

    <span class="k">def</span> <span class="nf">pdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">w</span><span class="o">*</span><span class="n">D</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span><span class="p">,</span> <span class="n">D</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">logpdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">rvs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="c1"># TODO: should randomize the output</span>
        <span class="n">sizes</span> <span class="o">=</span> <span class="n">partition</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">draws</span> <span class="o">=</span> <span class="p">[</span><span class="n">D</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">D</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="p">)]</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">draws</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">entropy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">10000</span><span class="p">):</span>
        <span class="c1"># No analytic expression, so estimate entropy using MC integration</span>
        <span class="k">return</span> <span class="n">entropy_mc</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">MultivariateT</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">sigma</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">sigma</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">sigma</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">if</span> <span class="n">mu</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">sigma</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mu</span><span class="p">))</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sigma</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">sigma</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mu</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">df</span> <span class="o">=</span> <span class="n">df</span>
        <span class="c1"># Use scipy stats to compute |Sigma| and (x-mu)^T Sigma^{-1} (x - mu),</span>
        <span class="c1"># and to estimate dimension p from rank.  Formula for pdf from wikipedia</span>
        <span class="c1"># https://en.wikipedia.org/wiki/Multivariate_t-distribution</span>
        <span class="kn">from</span> <span class="nn">scipy.stats._multivariate</span> <span class="kn">import</span> <span class="n">_PSD</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_psd</span> <span class="o">=</span> <span class="n">_PSD</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="p">)</span>
        <span class="n">nu</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">df</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_psd</span><span class="o">.</span><span class="n">rank</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_log_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">gammaln</span><span class="p">((</span><span class="n">nu</span> <span class="o">+</span> <span class="n">p</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
                          <span class="o">-</span> <span class="n">gammaln</span><span class="p">(</span><span class="n">nu</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
                          <span class="o">-</span> <span class="n">p</span><span class="o">/</span><span class="mi">2</span><span class="o">*</span><span class="n">log</span><span class="p">(</span><span class="n">pi</span><span class="o">*</span><span class="n">nu</span><span class="p">)</span>
                          <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_psd</span><span class="o">.</span><span class="n">log_pdet</span><span class="o">/</span><span class="mi">2</span>
                          <span class="p">)</span>

    <span class="k">def</span> <span class="nf">logpdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
        <span class="n">dev</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mu</span>
        <span class="n">maha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_psd</span><span class="o">.</span><span class="n">U</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">nu</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">df</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_psd</span><span class="o">.</span><span class="n">rank</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_log_norm</span> <span class="o">-</span> <span class="p">(</span><span class="n">nu</span><span class="o">+</span><span class="n">p</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log1p</span><span class="p">(</span><span class="n">maha</span><span class="o">/</span><span class="n">nu</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">pdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">rvs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="c1"># From farhawa on stack overflow</span>
        <span class="c1"># https://stackoverflow.com/questions/29798795/multivariate-student-t-distribution-with-python</span>
        <span class="n">nu</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">df</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mu</span><span class="p">)</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">nu</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="o">/</span><span class="n">nu</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">),</span> <span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">p</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">+</span> <span class="n">Z</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">entropy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">100000</span><span class="p">):</span>
        <span class="c1"># No analytic expression, so estimate entropy using MC integration</span>
        <span class="k">return</span> <span class="n">entropy_mc</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">MultivariateCauchy</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">MultivariateT</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Joint</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">distributions</span><span class="p">):</span>
        <span class="c1"># Note: list(x) converts any sequence, including generators, into a list</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">distributions</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">distributions</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">distributions</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">rvs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">D</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span> <span class="k">for</span> <span class="n">D</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">distributions</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">pdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">logpdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">D</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">theta</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">k</span><span class="p">])</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">D</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">distributions</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">cdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logcdf</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">logcdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">D</span><span class="o">.</span><span class="n">logcdf</span><span class="p">(</span><span class="n">theta</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">k</span><span class="p">])</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">D</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">distributions</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">sf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">expm1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logcdf</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">logsf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sf</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">entropy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">D</span><span class="o">.</span><span class="n">entropy</span><span class="p">()</span> <span class="k">for</span> <span class="n">D</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">distributions</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">partition</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="c1"># TODO: build an efficient algorithm for splitting n things into k buckets</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;i&#39;</span><span class="p">)</span>
    <span class="n">choices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">w</span><span class="p">)</span>
    <span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;f&#39;</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span>
    <span class="n">sizes</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">choices</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sizes</span>

<span class="k">def</span> <span class="nf">_check_entropy</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">N_entropy</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">N_norm</span><span class="o">=</span><span class="mi">2500</span><span class="p">,</span> <span class="n">demo</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Check if entropy from a random draw matches analytic entropy.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">use_kramer</span> <span class="o">=</span> <span class="n">use_mvn</span> <span class="o">=</span> <span class="n">use_wnn</span> <span class="o">=</span> <span class="n">use_gmm</span> <span class="o">=</span> <span class="n">use_kde</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">demo</span><span class="p">:</span>
        <span class="c1">#use_kramer = True</span>
        <span class="c1">#use_wnn = True</span>
        <span class="n">use_mvn</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">use_gmm</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">use_kde</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">use_kramer</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">get_state</span><span class="p">()</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="s1">&#39;dim&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">use_kramer</span><span class="p">:</span>
            <span class="n">logp_theta</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
            <span class="n">logp_theta</span> <span class="o">+=</span> <span class="mi">27</span>  <span class="c1"># result should be independent of scale factor</span>
            <span class="n">S</span><span class="p">,</span> <span class="n">Serr</span> <span class="o">=</span> <span class="n">entropy</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">logp_theta</span><span class="p">,</span> <span class="n">N_entropy</span><span class="o">=</span><span class="n">N_entropy</span><span class="p">,</span> <span class="n">N_norm</span><span class="o">=</span><span class="n">N_norm</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">use_wnn</span><span class="p">:</span>
            <span class="n">S_wnn</span><span class="p">,</span> <span class="n">Serr_wnn</span> <span class="o">=</span> <span class="n">wnn_entropy</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n_est</span><span class="o">=</span><span class="n">N_entropy</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">use_gmm</span><span class="p">:</span>
            <span class="n">S_gmm</span><span class="p">,</span> <span class="n">Serr_gmm</span> <span class="o">=</span> <span class="n">gmm_entropy</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n_est</span><span class="o">=</span><span class="n">N_entropy</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">use_mvn</span><span class="p">:</span>
            <span class="n">M</span> <span class="o">=</span> <span class="n">MVNEntropy</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
            <span class="n">S_mvn</span> <span class="o">=</span> <span class="n">M</span><span class="o">.</span><span class="n">entropy</span>
        <span class="k">if</span> <span class="n">use_kde</span><span class="p">:</span>
            <span class="n">S_kde</span> <span class="o">=</span> <span class="n">kde_entropy_statsmodels</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n_est</span><span class="o">=</span><span class="n">N_entropy</span><span class="p">)</span>
            <span class="c1">#S_kde = kde_entropy_sklearn(theta, n_est=N_entropy)</span>
            <span class="c1">#S_kde = kde_entropy_sklearn_gmm(theta, n_est=N_entropy)</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
    <span class="n">H</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">entropy</span><span class="p">()</span><span class="o">/</span><span class="n">LN2</span>
    <span class="k">if</span> <span class="n">demo</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;entropy&quot;</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="s2">&quot;~&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">use_kramer</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot; Kramer&quot;</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Serr</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">use_wnn</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot; wnn&quot;</span><span class="p">,</span> <span class="n">S_wnn</span><span class="p">,</span> <span class="n">Serr_wnn</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">use_gmm</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot; gmm&quot;</span><span class="p">,</span> <span class="n">S_gmm</span><span class="p">,</span> <span class="n">Serr_gmm</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">use_mvn</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot; MVN&quot;</span><span class="p">,</span> <span class="n">S_mvn</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">use_kde</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot; KDE&quot;</span><span class="p">,</span> <span class="n">S_kde</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">use_kramer</span><span class="p">:</span>
            <span class="c1">#assert Serr &lt; 0.05*S, &quot;incorrect error est. for Kramer&quot;</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">H</span><span class="p">):</span>
                <span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">S</span> <span class="o">-</span> <span class="n">H</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="o">*</span><span class="n">Serr</span><span class="p">,</span> <span class="s2">&quot;incorrect est. for Kramer&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">assert</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">S</span><span class="p">),</span> <span class="s2">&quot;incorrect est. for Kramer&quot;</span>
        <span class="k">if</span> <span class="n">use_wnn</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">H</span><span class="p">):</span>
                <span class="k">assert</span> <span class="n">Serr_wnn</span> <span class="o">&lt;</span> <span class="mf">0.05</span><span class="o">*</span><span class="n">S_wnn</span><span class="p">,</span> <span class="s2">&quot;incorrect error est. for wnn&quot;</span>
                <span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">S_wnn</span> <span class="o">-</span> <span class="n">H</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="o">*</span><span class="n">Serr_wnn</span><span class="p">,</span> <span class="s2">&quot;incorrect est. for wnn&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">assert</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">S_wnn</span><span class="p">),</span> <span class="s2">&quot;incorrect est. for wnn&quot;</span>

<span class="k">def</span> <span class="nf">_show_entropy</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">Timer</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">_check_entropy</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">demo</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_check_smoke</span><span class="p">(</span><span class="n">D</span><span class="p">):</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="s1">&#39;dim&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">logp_theta</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="n">entropy</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">logp_theta</span><span class="p">)</span>
    <span class="n">wnn_entropy</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="n">MVNEntropy</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span><span class="o">.</span><span class="n">entropy</span>

<span class="k">def</span> <span class="nf">test</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;check entropy estimates from known distributions&quot;&quot;&quot;</span>
    <span class="c1"># entropy test is optional: don&#39;t test if sklearn is not installed</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="kn">import</span> <span class="nn">sklearn</span>
    <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="c1"># Smoke test - do all the methods run in 1-D and 10-D?</span>
    <span class="n">_check_smoke</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">stats</span><span class="p">,</span> <span class="s1">&#39;multivariate_normal&#39;</span><span class="p">):</span>
        <span class="n">_check_smoke</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">cov</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">10</span><span class="p">)))</span>

    <span class="n">D</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
    <span class="n">_check_entropy</span><span class="p">(</span><span class="s2">&quot;N[100,8]&quot;</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">2000</span><span class="p">)</span>
    <span class="n">_check_entropy</span><span class="p">(</span><span class="s2">&quot;N[100,8]&quot;</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">12000</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">stats</span><span class="p">,</span> <span class="s1">&#39;multivariate_normal&#39;</span><span class="p">):</span>
        <span class="n">D</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">cov</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="mf">0.2</span><span class="o">**</span><span class="mi">2</span><span class="p">]))</span>
        <span class="n">_check_entropy</span><span class="p">(</span><span class="s2">&quot;MVN[1,12,0.2]&quot;</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>
        <span class="n">D</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">cov</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">10</span><span class="p">))</span>
        <span class="n">_check_entropy</span><span class="p">(</span><span class="s2">&quot;MVN[1]*10&quot;</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
        <span class="c1"># Make sure zero-width dimensions return H = -inf</span>
        <span class="n">D</span> <span class="o">=</span> <span class="n">MVNSingular</span><span class="p">(</span><span class="n">cov</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]))</span>
        <span class="n">_check_entropy</span><span class="p">(</span><span class="s2">&quot;MVN[1,1,0]&quot;</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
    <span class="c1">#raise TestFailure(&quot;make bumps testing fail so we know that test harness works&quot;)</span>

<span class="k">def</span> <span class="nf">mvn_entropy_test</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Test against results from the R MVN pacakge (using the web version)</span>
<span class="sd">    and the matlab Mskekur program (using Octave), both of which produce</span>
<span class="sd">    the same value.  Note that MVNEntropy uses the small sample correction</span>
<span class="sd">    for the skewness stat since it converges to the large sample value for</span>
<span class="sd">    large n.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
        <span class="p">[</span><span class="mf">2.4</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">4.5</span><span class="p">,</span> <span class="mf">4.9</span><span class="p">,</span> <span class="mf">5.7</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">3.5</span><span class="p">,</span> <span class="mf">1.8</span><span class="p">,</span> <span class="mf">3.9</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">3.9</span><span class="p">,</span> <span class="mf">4.7</span><span class="p">,</span> <span class="mf">4.7</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">6.7</span><span class="p">,</span> <span class="mf">3.6</span><span class="p">,</span> <span class="mf">5.9</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">3.6</span><span class="p">,</span> <span class="mf">2.9</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">5.3</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">,</span> <span class="mf">6.1</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">5.7</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">,</span> <span class="mf">6.2</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">5.2</span><span class="p">,</span> <span class="mf">4.1</span><span class="p">,</span> <span class="mf">6.4</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">2.4</span><span class="p">,</span> <span class="mf">2.9</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">3.2</span><span class="p">,</span> <span class="mf">2.7</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">2.7</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">4.1</span><span class="p">],</span>
    <span class="p">])</span>
    <span class="n">M</span> <span class="o">=</span> <span class="n">MVNEntropy</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="c1">#print(M)</span>
    <span class="c1">#print(&quot;%.15g %.15g %.15g&quot;%(M.p_kurtosis, M.p_skewness, M.entropy))</span>
    <span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">M</span><span class="o">.</span><span class="n">p_kurtosis</span> <span class="o">-</span> <span class="mf">0.265317890462476</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mf">1e-10</span>
    <span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">M</span><span class="o">.</span><span class="n">p_skewness</span> <span class="o">-</span> <span class="mf">0.773508066109368</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mf">1e-10</span>
    <span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">M</span><span class="o">.</span><span class="n">entropy</span> <span class="o">-</span> <span class="mf">5.7920040570988</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mf">1e-10</span>

    <span class="c1">## wnn_entropy doesn&#39;t work for small sample sizes (no surprise there!)</span>
    <span class="c1">#S_wnn, Serr_wnn = wnn_entropy(x)</span>
    <span class="c1">#assert abs(S_wnn - 5.7920040570988) &lt;= 1e-10</span>
    <span class="c1">#print(&quot;wnn %.15g, target %g&quot;%(S_wnn, 5.7920040570988))</span>

<span class="k">def</span> <span class="nf">demo</span><span class="p">():</span>
    <span class="c1"># hide module load time from Timer</span>
    <span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">NearestNeighbors</span>

    <span class="c1">## Bootstrap didn&#39;t help, but leave the test code in place for now</span>
    <span class="c1">#D = Dirichlet(alpha=[0.02]*20)</span>
    <span class="c1">#theta = D.rvs(size=1000)</span>
    <span class="c1">#S, Serr = wnn_bootstrap(D.rvs(size=200000))</span>
    <span class="c1">#print(&quot;bootstrap&quot;, S, D.entropy())</span>
    <span class="c1">#return</span>
    <span class="k">if</span> <span class="kc">False</span><span class="p">:</span>
        <span class="c1"># Multivariate T distribution</span>
        <span class="n">D</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
        <span class="n">_show_entropy</span><span class="p">(</span><span class="s2">&quot;T;df=4&quot;</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">20000</span><span class="p">)</span>
        <span class="n">D</span> <span class="o">=</span> <span class="n">MultivariateT</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mi">1</span><span class="p">]),</span> <span class="n">df</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
        <span class="n">_show_entropy</span><span class="p">(</span><span class="s2">&quot;MT[1];df=4&quot;</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">20000</span><span class="p">)</span>
        <span class="n">D</span> <span class="o">=</span> <span class="n">MultivariateT</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
        <span class="n">_show_entropy</span><span class="p">(</span><span class="s2">&quot;MT[1,12,0.2];df=4&quot;</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
        <span class="n">D</span> <span class="o">=</span> <span class="n">MultivariateT</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">10</span><span class="p">),</span> <span class="n">df</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
        <span class="n">_show_entropy</span><span class="p">(</span><span class="s2">&quot;MT[1]*10;df=4&quot;</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
        <span class="n">D</span> <span class="o">=</span> <span class="n">MultivariateT</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">1e2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
        <span class="n">_show_entropy</span><span class="p">(</span><span class="s2">&quot;MT[1,12,0.2,1e3,1e-3,1];df=4&quot;</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="k">if</span> <span class="kc">False</span><span class="p">:</span>
        <span class="c1"># Multivariate skew normal distribution</span>
        <span class="n">D</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">skewnorm</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
        <span class="n">_show_entropy</span><span class="p">(</span><span class="s2">&quot;skew=5 N[1]&quot;</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">20000</span><span class="p">)</span>
        <span class="n">D</span> <span class="o">=</span> <span class="n">Joint</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">skewnorm</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">])</span>
        <span class="n">_show_entropy</span><span class="p">(</span><span class="s2">&quot;skew=5 N[1,12,0.2]&quot;</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
        <span class="n">D</span> <span class="o">=</span> <span class="n">Joint</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">skewnorm</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">10</span><span class="p">)</span>
        <span class="n">_show_entropy</span><span class="p">(</span><span class="s2">&quot;skew=5 N[1]*10&quot;</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
        <span class="n">D</span> <span class="o">=</span> <span class="n">Joint</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">skewnorm</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">1e2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">_show_entropy</span><span class="p">(</span><span class="s2">&quot;skew=5 N[1,12,0.2,1e3,1e-3,1]&quot;</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
        <span class="c1">#print(&quot;double check entropy&quot;, D.entropy()/LN2, entropy_mc(D)/LN2)</span>
        <span class="k">return</span>

    <span class="n">D</span> <span class="o">=</span> <span class="n">Box</span><span class="p">(</span><span class="n">center</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">]</span><span class="o">*</span><span class="mi">10</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">_show_entropy</span><span class="p">(</span><span class="s2">&quot;Box 10!&quot;</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
    <span class="n">D</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
    <span class="c1">#_show_entropy(&quot;N[100,8]&quot;, D, N=100)</span>
    <span class="c1">#_show_entropy(&quot;N[100,8]&quot;, D, N=200)</span>
    <span class="c1">#_show_entropy(&quot;N[100,8]&quot;, D, N=500)</span>
    <span class="c1">#_show_entropy(&quot;N[100,8]&quot;, D, N=1000)</span>
    <span class="c1">#_show_entropy(&quot;N[100,8]&quot;, D, N=2000)</span>
    <span class="c1">#_show_entropy(&quot;N[100,8]&quot;, D, N=5000)</span>
    <span class="n">_show_entropy</span><span class="p">(</span><span class="s2">&quot;N[100,8]&quot;</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
    <span class="c1">#_show_entropy(&quot;N[100,8]&quot;, D, N=20000)</span>
    <span class="c1">#_show_entropy(&quot;N[100,8]&quot;, D, N=50000)</span>
    <span class="c1">#_show_entropy(&quot;N[100,8]&quot;, D, N=100000)</span>
    <span class="n">D</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">cov</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="c1">#_show_entropy(&quot;MVN[1,12,0.2]&quot;, D)</span>
    <span class="n">D</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">cov</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">10</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="c1">#_show_entropy(&quot;MVN[1]*10&quot;, D, N=1000)</span>
    <span class="n">_show_entropy</span><span class="p">(</span><span class="s2">&quot;MVN[1]*10&quot;</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
    <span class="c1">#_show_entropy(&quot;MVN[1]*10&quot;, D, N=100000)</span>
    <span class="c1">#_show_entropy(&quot;MVN[1]*10&quot;, D, N=200000, N_entropy=20000)</span>
    <span class="n">D</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">cov</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="c1">#_show_entropy(&quot;MVN[1,12,0.2,1,1,1]&quot;, D, N=100)</span>
    <span class="c1">#_show_entropy(&quot;MVN[1,12,0.2,1,1,1]&quot;, D, N=1000)</span>
    <span class="n">_show_entropy</span><span class="p">(</span><span class="s2">&quot;MVN[1,12,0.2,1,1,1]&quot;</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
    <span class="c1">#_show_entropy(&quot;MVN[1,12,0.2,1,1,1]&quot;, D, N=100000)</span>
    <span class="n">D</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">cov</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">1e2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="c1">#_show_entropy(&quot;MVN[1,12,0.2,1e3,1e-3,1]&quot;, D, N=100)</span>
    <span class="c1">#_show_entropy(&quot;MVN[1,12,0.2,1e3,1e-3,1]&quot;, D, N=1000)</span>
    <span class="n">_show_entropy</span><span class="p">(</span><span class="s2">&quot;MVN[1,12,0.2,1e3,1e-3,1]&quot;</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
    <span class="c1">#_show_entropy(&quot;MVN[1,12,0.2,1e3,1e-3,1]&quot;, D, N=100000)</span>
    <span class="n">D</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">],</span> <span class="n">mu</span><span class="o">=</span><span class="p">[[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="mi">10</span><span class="p">,</span> <span class="p">[</span><span class="mi">100</span><span class="p">]</span><span class="o">*</span><span class="mi">10</span><span class="p">],</span> <span class="n">sigma</span><span class="o">=</span><span class="p">[[</span><span class="mi">10</span><span class="p">]</span><span class="o">*</span><span class="mi">10</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">]</span><span class="o">*</span><span class="mi">10</span><span class="p">])</span>
    <span class="n">_show_entropy</span><span class="p">(</span><span class="s2">&quot;bimodal mixture&quot;</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>
    <span class="n">D</span> <span class="o">=</span> <span class="n">Dirichlet</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="p">[</span><span class="mf">0.02</span><span class="p">]</span><span class="o">*</span><span class="mi">20</span><span class="p">)</span>
    <span class="c1">#_show_entropy(&quot;Dirichlet[0.02]*20&quot;, D, N=1000)</span>
    <span class="c1">#_show_entropy(&quot;Dirichlet[0.02]*20&quot;, D, N=2000)</span>
    <span class="c1">#_show_entropy(&quot;Dirichlet[0.02]*20&quot;, D, N=5000)</span>
    <span class="c1">#_show_entropy(&quot;Dirichlet[0.02]*20&quot;, D, N=10000)</span>
    <span class="n">_show_entropy</span><span class="p">(</span><span class="s2">&quot;Dirichlet[0.02]*20&quot;</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">20000</span><span class="p">)</span>
    <span class="c1">#_show_entropy(&quot;Dirichlet[0.02]*20&quot;, D, N=50000)</span>
    <span class="c1">#_show_entropy(&quot;Dirichlet[0.02]*20&quot;, D, N=200000, N_entropy=20000)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>  <span class="c1"># pragma: no cover</span>
    <span class="n">demo</span><span class="p">()</span>
</pre></div>

            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../../index.html">
              <img class="logo" src="../../../_static/logo.png" alt="Logo"/>
            </a></p>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">Bumps 0.9.1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../index.html" >Module code</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../../bumps.html" >bumps</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">bumps.dream.entropy</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    </div>
  </body>
</html>